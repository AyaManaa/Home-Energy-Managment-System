{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d31764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np \n",
    "import os \n",
    "import random \n",
    "import torch as T \n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e9ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftEnv1(gym.Env):\n",
    "    \n",
    "    def __init__(self, reset_num_hour, appliance_hourly_power_consumption, hourly_total_power_consumption, max_on_times):\n",
    "        self.action_space= Discrete(2)\n",
    "        self.observation_space= Box(low=0, high= hourly_total_power_consumption, shape=(1,), dtype= float)\n",
    "        self.reset_num_hour= reset_num_hour\n",
    "        self.appliance_hourly_power_consumption= appliance_hourly_power_consumption\n",
    "        self.state= hourly_total_power_consumption # state is going to be the hourly power consumption\n",
    "        self.max_on_times= max_on_times\n",
    "        self.on_times= 0\n",
    "        self.curr_hour=0 \n",
    "        \n",
    "    def reset(self, seed= None, options= None):\n",
    "        self.curr_hour= 0 \n",
    "        self.on_times= 0 \n",
    "        return self.curr_hour, {}\n",
    "    \n",
    "    def step(self,action):\n",
    "        \n",
    "        reward= 0 \n",
    "        self.curr_hour+=1\n",
    "        \n",
    "        if action==1:# Appliance is turned ON \n",
    "            \n",
    "            \n",
    "            self.state -= self.appliance_hourly_power_consumption\n",
    "            self.on_times+=1\n",
    "            \n",
    "            if self.on_times > self.max_on_times:\n",
    "                reward= -15\n",
    "            else:\n",
    "                reward= 25\n",
    "        \n",
    "        else:# Appliance is turned OFF\n",
    "            #reward= -10\n",
    "            if self.curr_hour in [9, 10, 11, 12, 13, 14, 15, 16, 6, 7, 8]: # intermidate hours\n",
    "                reward= -10\n",
    "            elif self.curr_hour in [0, 1, 2, 3, 4, 5]: #off beak hours\n",
    "                reward= -5\n",
    "            elif self.curr_hour in [17, 18, 19, 20, 21, 22, 23]: # peak hours\n",
    "                reward= 25\n",
    "                \n",
    "            \n",
    "        \n",
    "        done = self.curr_hour == self.reset_num_hour\n",
    "        \n",
    "        if done:\n",
    "            self.curr_hour= 0\n",
    "        \n",
    "        next_state= round(self.state,4) # the new total power consumption is the next_state \n",
    "        \n",
    "        info = {'Action taken: ': action, 'Total Power Consumption ': self.state,\n",
    "                'Hour of the day': self.curr_hour, 'ON Times ': self.on_times}\n",
    "        #print(f\"Current hour: {info['Hour of the day']}, Action taken: {info['Action taken: ']}\\\n",
    "        #\\nTotal Power Consumption {info['Total Power Consumption ']}, ON Times: {self.on_times}\")\n",
    "        #print(\"_\"*20)\n",
    "        \n",
    "        return next_state, reward, done, info, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de39e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a0f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### New Reward Function \n",
    "class ShiftEnv2(Env):\n",
    "    \n",
    "    def __init__(self, reset_num_hour, appliance_hourly_power_consumption, hourly_total_power_consumption, max_on_times):\n",
    "        self.action_space= Discrete(2)\n",
    "        self.observation_space= Box(low=0, high= hourly_total_power_consumption, shape=(1,), dtype= float)\n",
    "        self.reset_num_hour= reset_num_hour\n",
    "        self.appliance_hourly_power_consumption= appliance_hourly_power_consumption\n",
    "        self.state= hourly_total_power_consumption # state is going to be the hourly power consumption\n",
    "        self.max_on_times= max_on_times\n",
    "        self.on_times= 0\n",
    "        self.curr_hour=0 \n",
    "        \n",
    "    def reset(self, seed= None, options= None):\n",
    "        self.curr_hour=0 \n",
    "        self.on_times= 0\n",
    "        return self.curr_hour, {}\n",
    "    \n",
    "    def step(self,action):\n",
    "        \n",
    "        reward= 0 \n",
    "        self.curr_hour+=1\n",
    "        \n",
    "        if action==1:# Appliance is turned ON \n",
    "            \n",
    "            \n",
    "            self.state -= self.appliance_hourly_power_consumption\n",
    "            self.on_times+=1\n",
    "            if self.on_times > self.max_on_times:\n",
    "                reward= -15\n",
    "            else:\n",
    "                reward= 25\n",
    "                \n",
    "            if self.curr_hour in [9, 10, 11, 12, 13, 14, 15, 16, 6, 7, 8]:# intermidate hours\n",
    "                \n",
    "                reward= 25\n",
    "                \n",
    "            elif self.curr_hour in [0, 1, 2, 3, 4, 5]: #off beak hours\n",
    "                reward= 15\n",
    "            \n",
    "            elif self.curr_hour in [17, 18, 19, 20, 21, 22, 23]: # peak hours\n",
    "                reward= -15\n",
    "    \n",
    "        else:# Appliance is turned OFF\n",
    "            #reward= -10\n",
    "            if self.curr_hour in [9, 10, 11, 12, 13, 14, 15, 16, 6, 7, 8]: # intermidate hours\n",
    "                reward= -15\n",
    "            elif self.curr_hour in [0, 1, 2, 3, 4, 5]: #off beak hours\n",
    "                reward= -10\n",
    "            elif self.curr_hour in [17, 18, 19, 20, 21, 22, 23]: # peak hours\n",
    "                reward= 25\n",
    "                \n",
    "            \n",
    "        \n",
    "        done = self.curr_hour == self.reset_num_hour\n",
    "        \n",
    "        if done:\n",
    "            self.curr_hour= 0\n",
    "        \n",
    "        next_state= round(self.state,4) # the new total power consumption is the next_state \n",
    "        \n",
    "        info = {'Action taken: ': action, 'Total Power Consumption ': self.state,\n",
    "                'Hour of the day': self.curr_hour}\n",
    "        #print(f\"Current hour: {info['Hour of the day']}, Action taken: {info['Action taken: ']}\\\n",
    "        #\\nTotal Power Consumption {info['Total Power Consumption ']}, ON Times: {self.on_times}\")\n",
    "        #print(\"_\"*20)\n",
    "        \n",
    "        return next_state, reward, done, info, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac93f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Perform any desired actions or data collection at each training step\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        return self.model, self.logger.episode_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bbf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09978863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(custom_env, lr, gamma, epsilon_dec, epsilon_min, log_path= None):\n",
    "          \n",
    "    policy_args= dict(net_arch=[256,256])# two hidden layers each with 256 neurons\n",
    "    \n",
    "    model= DQN(env= custom_env, policy= 'MlpPolicy', learning_rate=lr, gamma= gamma,\n",
    "               exploration_initial_eps= 1.0, exploration_fraction= epsilon_dec, \n",
    "               exploration_final_eps= epsilon_min, buffer_size= 100000, verbose= 1, \n",
    "               batch_size= 64, policy_kwargs= policy_args, tensorboard_log= log_path)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea568b4",
   "metadata": {},
   "source": [
    "# Testing Enviroment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca964f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env= ShiftEnv1(reset_num_hour=23, appliance_hourly_power_consumption=0.025,\n",
    "               hourly_total_power_consumption= 250, max_on_times=3)\n",
    "episodes= 48\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    score= 0\n",
    "    done= False\n",
    "    while not done:\n",
    "        action= env.action_space.sample()\n",
    "        next_state, reward, done, info, _= env.step(action)\n",
    "        score+=reward\n",
    "        state= next_state\n",
    "    #print(f'Time:{episode}\\nScore: {score}\\n Info: {info}')\n",
    "    print('+*'*20)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5236fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= ShiftEnv2(reset_num_hour=24, appliance_hourly_power_consumption=0.025,\n",
    "               hourly_total_power_consumption= 250, max_on_times=3)\n",
    "episodes= 48\n",
    "for episode in range(episodes):\n",
    "    state= env.reset()\n",
    "    score= 0\n",
    "    done= False\n",
    "    while not done:\n",
    "        action= env.action_space.sample()\n",
    "        next_state, reward, done, info, _= env.step(action)\n",
    "        score+=reward\n",
    "        state= next_state\n",
    "    #print(f'Time:{episode}\\nScore: {score}\\n Info: {info}')\n",
    "    print('+*'*20)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3629b",
   "metadata": {},
   "source": [
    "# Training Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55f4b1",
   "metadata": {},
   "source": [
    "## First Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5cb84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f9dbde2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PRO\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 876      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.67     |\n",
      "|    n_updates        | 193      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1752     |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 1752     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 0.649    |\n",
      "|    n_updates        | 412      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2628     |\n",
      "|    fps              | 711      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2628     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.99     |\n",
      "|    n_updates        | 631      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3504     |\n",
      "|    fps              | 669      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 3504     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.39     |\n",
      "|    n_updates        | 850      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4380     |\n",
      "|    fps              | 640      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4380     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.92     |\n",
      "|    n_updates        | 1069     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5256     |\n",
      "|    fps              | 645      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 5256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 2.62     |\n",
      "|    n_updates        | 1288     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6132     |\n",
      "|    fps              | 655      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 6132     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 3.18     |\n",
      "|    n_updates        | 1507     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 7008     |\n",
      "|    fps              | 634      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 7008     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 3.74     |\n",
      "|    n_updates        | 1726     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 7884     |\n",
      "|    fps              | 626      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 7884     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 4.4      |\n",
      "|    n_updates        | 1945     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8760     |\n",
      "|    fps              | 627      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 8760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 4.79     |\n",
      "|    n_updates        | 2164     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 9636     |\n",
      "|    fps              | 616      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 9636     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 5.38     |\n",
      "|    n_updates        | 2383     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 7        |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 10512    |\n",
      "|    fps              | 612      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 10512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.23e+03 |\n",
      "|    n_updates        | 2602     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 11388    |\n",
      "|    fps              | 603      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 11388    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.34e+03 |\n",
      "|    n_updates        | 2821     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12264    |\n",
      "|    fps              | 599      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 12264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.43e+03 |\n",
      "|    n_updates        | 3040     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 13140    |\n",
      "|    fps              | 595      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 13140    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.11e+03 |\n",
      "|    n_updates        | 3259     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 14016    |\n",
      "|    fps              | 589      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 14016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.36e+03 |\n",
      "|    n_updates        | 3478     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 14892    |\n",
      "|    fps              | 590      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 14892    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.23e+03 |\n",
      "|    n_updates        | 3697     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 15768    |\n",
      "|    fps              | 587      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 15768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.07e+03 |\n",
      "|    n_updates        | 3916     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16644    |\n",
      "|    fps              | 587      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 16644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.12e+03 |\n",
      "|    n_updates        | 4135     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 17520    |\n",
      "|    fps              | 582      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 17520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.14e+03 |\n",
      "|    n_updates        | 4354     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "env= ShiftEnv1(reset_num_hour=24, appliance_hourly_power_consumption=0.025,\n",
    "               hourly_total_power_consumption= 250, max_on_times=3)\n",
    "log_path= os.path.join('ShiftableDQN', 'Env1')\n",
    "eval_callback= EvalCallback(eval_env= env,log_path= log_path, deterministic=True)\n",
    "# increasing training timesteps\n",
    "model= agent(custom_env= env, lr= 0.03, gamma= 0.99,epsilon_dec= 0.05, epsilon_min= 0.01)\n",
    "model.learn(total_timesteps=17520, log_interval=876)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46cf83fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([item['r'] for item in model.ep_info_buffer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af525aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params= {\n",
    "    'lr': [0.03, 0.02, 0.01],\n",
    "    'gamma': [0.99, 0.85, 0.80, 0.70],\n",
    "    'eps_min': [0.01, 0.1, 0.15],\n",
    "    'eps_dec': [0.05, 0.15, 0.25, 0.50, 0.75, 0.85],\n",
    "}\n",
    "best_reward=0\n",
    "best_params= {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d74c2",
   "metadata": {},
   "source": [
    "for lr in agent_params['lr']:\n",
    "    for gamma in agent_params['gamma']:\n",
    "        for eps_min in agent_params['eps_min']:\n",
    "            for eps_dec in agent_params['eps_dec']:\n",
    "                \n",
    "                env= ShiftEnv1(reset_num_hour=24, appliance_hourly_power_consumption=0.025,\n",
    "                               hourly_total_power_consumption= 250, max_on_times=3)\n",
    "                model= agent(custom_env= env, lr= lr, gamma= gamma,\n",
    "                             epsilon_dec= eps_dec, epsilon_min= eps_min)\n",
    "                \n",
    "                print('Parameters\\n')\n",
    "                print(f'Learning Rate: {lr}, Gamma: {gamma}, Epsilon Min: {eps_min}, Epsilon Decrement: {eps_dec}')\n",
    "                model.learn(total_timesteps= 4320, log_interval=24)\n",
    "                \n",
    "                episode_reward_mean= np.mean([item['r'] for item in model.ep_info_buffer])\n",
    "                if episode_reward_mean > best_reward:\n",
    "                    best_reward= episode_reward_mean\n",
    "                    best_params= {\n",
    "                        'lr': lr,\n",
    "                        'gamma': gamma,\n",
    "                        'eps_min': eps_min,\n",
    "                        'eps_dec': eps_dec\n",
    "                    }\n",
    "                    print(f'BEST REWARD: {best_reward}\\nPARAMETERS: {best_params}')\n",
    "                    \n",
    "                \n",
    "                print('+*'*40)\n",
    "                env.close()\n",
    "\n",
    "\n",
    "print(f'BEST REWARD: {best_reward}\\n')\n",
    "print(f'BEST PARAMS: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d45395e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 809      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 876      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 0.76     |\n",
      "|    n_updates        | 193      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1752     |\n",
      "|    fps              | 733      |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 1752     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.36     |\n",
      "|    n_updates        | 412      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2628     |\n",
      "|    fps              | 673      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 2628     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 2.63     |\n",
      "|    n_updates        | 631      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3504     |\n",
      "|    fps              | 650      |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 3504     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.81     |\n",
      "|    n_updates        | 850      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4380     |\n",
      "|    fps              | 630      |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 4380     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 2.67     |\n",
      "|    n_updates        | 1069     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5256     |\n",
      "|    fps              | 617      |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 5256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 3        |\n",
      "|    n_updates        | 1288     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6132     |\n",
      "|    fps              | 603      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 6132     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 2.87     |\n",
      "|    n_updates        | 1507     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 7008     |\n",
      "|    fps              | 602      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 7008     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 3.81     |\n",
      "|    n_updates        | 1726     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 7884     |\n",
      "|    fps              | 610      |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 7884     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 4.09     |\n",
      "|    n_updates        | 1945     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8760     |\n",
      "|    fps              | 612      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 8760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 5.05     |\n",
      "|    n_updates        | 2164     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 9636     |\n",
      "|    fps              | 612      |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 9636     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 5.83     |\n",
      "|    n_updates        | 2383     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | -4.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 10512    |\n",
      "|    fps              | 613      |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 10512    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.33e+03 |\n",
      "|    n_updates        | 2602     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 11388    |\n",
      "|    fps              | 611      |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 11388    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.5e+03  |\n",
      "|    n_updates        | 2821     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12264    |\n",
      "|    fps              | 610      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 12264    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.34e+03 |\n",
      "|    n_updates        | 3040     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 13140    |\n",
      "|    fps              | 613      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 13140    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.53e+03 |\n",
      "|    n_updates        | 3259     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 14016    |\n",
      "|    fps              | 615      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 14016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.32e+03 |\n",
      "|    n_updates        | 3478     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 14892    |\n",
      "|    fps              | 619      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 14892    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.18e+03 |\n",
      "|    n_updates        | 3697     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 15768    |\n",
      "|    fps              | 617      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 15768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.33e+03 |\n",
      "|    n_updates        | 3916     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16644    |\n",
      "|    fps              | 612      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 16644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.08e+03 |\n",
      "|    n_updates        | 4135     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.01     |\n",
      "| time/               |          |\n",
      "|    episodes         | 17520    |\n",
      "|    fps              | 611      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 17520    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.03     |\n",
      "|    loss             | 1.16e+03 |\n",
      "|    n_updates        | 4354     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "env= ShiftEnv1(reset_num_hour=24, appliance_hourly_power_consumption=0.025,\n",
    "               hourly_total_power_consumption= 250, max_on_times=3)\n",
    "log_path= os.path.join('ShiftableDQN', 'Env1')\n",
    "eval_callback= EvalCallback(eval_env= env,log_path= log_path, deterministic=True)\n",
    "# increasing training timesteps\n",
    "model_env1= agent(custom_env= env, lr= 0.03, gamma= 0.99,epsilon_dec= 0.05, epsilon_min= 0.01)\n",
    "model_env1.learn(total_timesteps=17520, log_interval=876)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484404bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([item['r'] for item in model_env1.ep_info_buffer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcae731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env1.save('env1_shiftable_dqn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d5c71",
   "metadata": {},
   "source": [
    "## Second Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256081c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_params= {\n",
    "    'lr': [0.03, 0.02, 0.01],\n",
    "    'gamma': [0.99, 0.85, 0.80, 0.70],\n",
    "    'eps_min': [0.01, 0.1, 0.15],\n",
    "    'eps_dec': [0.05, 0.15, 0.25, 0.50, 0.75, 0.85],\n",
    "}\n",
    "best_reward=0\n",
    "best_params= {}\n",
    "\n",
    "best_reward=0\n",
    "best_params= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f16a9a",
   "metadata": {},
   "source": [
    "for lr in agent_params['lr']:\n",
    "    for gamma in agent_params['gamma']:\n",
    "        for eps_dec in agent_params['eps_dec']:\n",
    "            for eps_min in agent_params['eps_min']:\n",
    "                \n",
    "                env= ShiftEnv2(reset_num_hour=24, appliance_hourly_power_consumption=0.025,\n",
    "                               hourly_total_power_consumption= 250, max_on_times=3)\n",
    "                model= agent(custom_env= env, lr= lr, gamma= gamma, epsilon_dec= eps_dec, epsilon_min= eps_min)\n",
    "                model.learn(total_timesteps= 4320, log_interval= 24)\n",
    "                \n",
    "                episode_reward_mean= np.mean([item['r'] for item in model.ep_info_buffer])\n",
    "                if episode_reward_mean > best_reward:\n",
    "                    \n",
    "                    best_reward= episode_reward_mean\n",
    "                    best_params= {\n",
    "                        'lr': lr,\n",
    "                        'gamma': gamma,\n",
    "                        'eps_min': eps_min,\n",
    "                        'eps_dec': eps_dec\n",
    "                    }\n",
    "                    print(f'BEST REWARD: {best_reward}\\nPARAMETERS: {best_params}')\n",
    "                \n",
    "                print('+*'*40)\n",
    "                env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'BEST REWARD: {best_reward}\\n')\n",
    "print(f'BEST PARAMS: {best_params}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637ab08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env= ShiftEnv2(reset_num_hour=24, appliance_hourly_power_consumption=0.025,hourly_total_power_consumption= 250, max_on_times=3)\n",
    "\n",
    "log_path= os.path.join('ShiftableDQN', 'Env2')\n",
    "eval_callback= EvalCallback(eval_env= env,log_path= log_path, deterministic=True)\n",
    "model_env2= agent(custom_env=env, lr= 0.03, gamma= 0.99, epsilon_min= 0.01, epsilon_dec= 0.5)\n",
    "model_env2.learn(total_timesteps=17520, log_interval=876)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8ffc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean([item['r'] for item in model_env2.ep_info_buffer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453e399",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_path= os.path.join('Shiftable Agent Deployment', 'Enviromnent1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21730238",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env1.save(env1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c42568",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2_path= os.path.join('Shiftable Agent Deployment', 'Enviromnent2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcca6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_env2.save(env2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_env2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env2= DQN.load('Shiftable Agent Deployment\\Enviromnent2.zip', shift_env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1964b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp_venv",
   "language": "python",
   "name": "gp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
